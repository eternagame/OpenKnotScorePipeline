{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data for Sherlock Processing\n",
    "\n",
    "If you plan to generate in-silico predictions on Sherlock and you have lots of data to process (thousands+), it's highly recommended that you split the data into subsets and run the structure prediction scripts in parallel on many nodes using a batch array. See 2.GeneratePredictions.ipynb for more details. \n",
    "\n",
    "This notebook lets you define how many subset files you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# If true, read in data+predictions.pkl and include previously-generated predictions in the\n",
    "# newly-generated subsets. This is useful if, for example, you've previously run predictions\n",
    "# but need to extend them with additional metadata\n",
    "MERGE = True\n",
    "\n",
    "job = input(\n",
    "    \"Job Name: whatever you'd like to call this processing run. We'll be storing the subset \" +\n",
    "    \"files in SCRATCH here, and we need to fetch them later in the processing script\"\n",
    ")\n",
    "subsetSize = int(input(\n",
    "    \"Subset Size: how many rows you'd like in each subset file. Total number generated is \" +\n",
    "    \"rowsToProcess / subsetSize.\"\n",
    "))\n",
    "\n",
    "# Pick your input file according to whether you are processing the whole dataset or just high quality sequences\n",
    "inputFile = f'../data/data_rdatOnly.pkl' \n",
    "# inputFile = f'../data/data_highQuality.pkl' \n",
    "outputDataDir = f'{os.environ[\"SCRATCH\"]}/{job}/data'\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(outputDataDir, exist_ok=True)\n",
    "\n",
    "# Grab the data\n",
    "data = pd.read_pickle(inputFile)\n",
    "rowsToProcess = data.shape[0]\n",
    "\n",
    "if MERGE:\n",
    "    processed_data = pd.read_pickle('../data/data+predictions.pkl')\n",
    "    data = data.join(processed_data[[col for col in processed_data.columns if col.endswith('_PRED') or col.endswith('_time')]])\n",
    "\n",
    "# Loop over the data to generate subset files\n",
    "for (i, index) in enumerate(range(0, rowsToProcess, subsetSize)):\n",
    "    subset = data[index:index+subsetSize]\n",
    "    subset.to_pickle(f'{outputDataDir}/{i:03}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Test to make sure that the output subsets are what you expected \n",
    "test = pd.read_pickle(f'{outputDataDir}/000.pkl')\n",
    "display(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
